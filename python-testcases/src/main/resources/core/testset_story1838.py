'''
COPYRIGHT Ericsson 2019
The copyright to the computer program(s) herein is the property of
Ericsson Inc. The programs may be used and/or copied only with written
permission from Ericsson Inc. or in accordance with the terms and
conditions stipulated in the agreement/contract under which the
program(s) have been supplied.

@since:     April 2014
@author:    Luke Murphy
@summary:   Integration test for HA node lock/unlock tasks
            Story: 1838 Sub-Task: 3929
'''

from litp_generic_test import GenericTest, attr
from redhat_cmd_utils import RHCmdUtils
import test_constants
import os
LOCAL_PLUGINS_DIR = os.path.abspath(
                        os.path.join(os.path.dirname(__file__), 'plugins'))

# Node locking/unlocking tasks can be generated by only one plugin, usually
# done by the HA plugin (VCS/CMW). Attempting to create a lock/unlock task from
# a plugin api test will fail, in the positive cases. This cannot be run on an
# HA cluster, just a generic cluster.
# The negative cases where an error is returned, when two plugins try to create
# lock/unlock tasks at the same time, should all still pass on an HA cluster as
# it proves that only the HA plugin is allowed to return lock/unlock tasks.
# Based on this, and that we no longer use generic clusters, the positive cases
# are now obsolete, while the negative will remain to ensure robustness.
# These tests haven't been executed in some time, a lot of code changes since,
# if we install a generic cluster at some point, some tests may required
# updates.


class Story1838(GenericTest):
    """As a litp admin I want litp to provide a way of
       locking nodes so that I can perform upgrades."""

    def setUp(self):
        """Runs before every test to perform setup"""
        super(Story1838, self).setUp()
        self.test_ms = self.get_management_node_filename()
        self.orig_ha_manager = ""
        self.rhcmd = RHCmdUtils()
        self.plugin_id = "story1838"
        self.log_path = test_constants.GEN_SYSTEM_LOG_PATH

    def tearDown(self):
        """Runs after every test to perform cleanup"""
        super(Story1838, self).tearDown()
        self._uninstall_rpms()

    def update_ha_manager_prop(self, new_value="", reset=False):
        """Update the HA manager property for the given cluster url. If
           'reset' if True, then we want to set it back to the original
           value which is None (run at end of each test to preserve model)"""
        ha_manager_url = self.find_children_of_collect(
            self.test_ms, "/deployments", "cluster"
        )[0]
        if reset:
            # reset value back to original value that was stored in
            # instance variable
            self.execute_cli_update_cmd(
                self.test_ms, ha_manager_url,
                props="ha_manager={0}".format(self.orig_ha_manager)
            )
        else:
            # save old value in an instance variable
            # if we get a TypeError, the ha_manager property is not present
            try:
                self.orig_ha_manager = self.get_props_from_url(
                    self.test_ms, ha_manager_url, "ha_manager"
                )
            except TypeError:
                # we will reset to empty string because it was never set
                self.orig_ha_manager = ""

            # in the case we do not get a TypeError, but we return
            # nothing, then 'None' is not a valid value for properties
            if self.orig_ha_manager == None:
                self.orig_ha_manager = ""

            # update with new value
            self.execute_cli_update_cmd(
                self.test_ms, ha_manager_url,
                props="ha_manager={0}".format(new_value)
            )
        return True

    @staticmethod
    def get_local_rpm_paths(path, rpm_substring):
        """given a path (which should contain some RPMs) and a substring
           which is present in the RPM names you want, return a list of
           absolute paths to the RPMS that are local to your test"""
        # get all RPMs in 'path' that contain 'rpm_substring' in their name
        rpm_names = [rpm for rpm in os.listdir(path) if rpm_substring in rpm]

        if not rpm_names:
            return None

        # return a list of absolute paths to the RPMs found in 'rpm_names'
        return [
            os.path.join(rpath, rpm)
            for rpath, rpm in
            zip([os.path.abspath(path)] * len(rpm_names), rpm_names)
        ]

    def get_software_path(self, plugin_id):
        """Build create command using string 'plugin_id'
           as the id of the path"""
        return os.path.join(
            self.find(self.test_ms, "/software", "software-item", False)[0],
            plugin_id
        )

    def get_lock_unlock_strings(self, nodes):
        """Helper method to return two lists containing strings
           that are expected in plan output"""
        # get list of managed nodes filenames
        node_fnames = []
        for node in nodes:
            node_fnames.append(
                self.get_node_filename_from_url(self.test_ms, node)
            )

        # get node hostnames
        node_hnames = [
            self.get_node_att(node, "hostname") for node in node_fnames
        ]

        # build strings that we will expect in the plan output
        lock_str = ["Lock task {0}".format(hname) for hname in node_hnames]
        unlock_str = [
            "Unlock task {0}".format(hname) for hname in node_hnames
        ]

        # if only one node was passed for 'nodes', then we only want
        # to return strings
        if len(lock_str) == 1:
            lock_str = lock_str[0]

        if len(unlock_str) == 1:
            unlock_str = unlock_str[0]

        return lock_str, unlock_str

    def lock_unlock_tasks_correct_order(self, plan_output, nodes):
        """Given a list of output from a 'show_plan' CLI command and a
           list of nodes which should have lock/unlock tasks, check
           that the node lock/unlock tasks are in the correct order"""
        for node in nodes:
            lock_str, unlock_str = self.get_lock_unlock_strings([node])
            self.log(
                "info",
                "{0} at {1} and {2} at {3}".format(
                    lock_str, plan_output.index(lock_str),
                    unlock_str, plan_output.index(unlock_str)
                )
            )
            self.assertTrue(
                plan_output.index(lock_str) < plan_output.index(unlock_str)
            )
        return True

    def install_rpms(self, node, local_rpm_dir, rpm_filter):
        """wrapper function to install RPMS"""
        # since the copy_and_install_rpms method in the framework, doesn't
        # check if the package is already installed, we must check if the
        # package does indeed need to be installed - if we don't, and the
        # package is installed, the test will fail
        rpms = self.get_local_rpm_path_ls(local_rpm_dir, rpm_filter)
        rpms_to_install = []
        for rpm in rpms:
            pkg_name = (os.path.basename(rpm)).rstrip('.rpm')
            is_pkg_installed = self.check_pkgs_installed(node,
                                                         [pkg_name],
                                                         su_root=True)
            if not is_pkg_installed:
                rpms_to_install.append(rpm)
        if rpms_to_install:
            self.assertTrue(self.copy_and_install_rpms(node, rpms_to_install))

    def _uninstall_rpms(self):
        """
        Description:
            Method that uninstalls plugin and extension on
            the MS.
        """
        local_rpm_paths = self.get_local_rpm_paths(
            os.path.join(
                os.path.dirname(repr(__file__).strip("'")), "plugins"
                ),
            self.plugin_id
            )

        if local_rpm_paths:
            installed_rpm_names = []
            for rpm in local_rpm_paths:
                std_out, std_err, rc = self.run_command_local(
                    self.rhc.get_package_name_from_rpm(rpm))
                self.assertEquals(0, rc)
                self.assertEquals([], std_err)
                self.assertEquals(1, len(std_out))

                installed_rpm_names.append(std_out[0])

            self.run_command(self.test_ms,
                                        self.rhc.get_yum_remove_cmd(
                                            installed_rpm_names),
                                        add_to_cleanup=False,
                                        default_asserts=True,
                                        su_root=True
                                       )

    def _get_expected_log_message(self, expected_message, test_log_len=-1):
        """get expected message from /var/log/messages from plugin execution"""

        # check /var/log/messages for the expected message
        if test_log_len != -1:
            stdout, stderr, rcode = self.run_command(
                self.test_ms,
                self.rhc.get_grep_file_cmd(
                    self.log_path, [expected_message],
                    file_access_cmd='/usr/bin/tail -n {0}'.format(test_log_len)
                )
            )
        else:
            stdout, stderr, rcode = self.run_command(
                self.test_ms,
                self.rhc.get_grep_file_cmd(self.log_path, [expected_message])
            )
        self.assertEqual(0, rcode)
        self.assertNotEqual([], stdout)
        self.assertEqual([], stderr)

    #attr('all', 'non-revert')
    def obsolete_01_p_required_tasks(self):
        """
            Description:
                Given a LITP deployment, if a node is part of a HA_cluster
                and the plan contains at least one task for that node, if the
                plugin provides lock/unlock tasks, then the task(s) will be
                executed while the node is locked (the node will not
                necessarily be unlocked immediately after
                the task(s) is executed).
            Actions:
                1. Copy over and install test RPMS
                2. Run create + link commands
                3. Run create_plan
                4. Assert that the output is as expected
                5. Remove plan
                6. Run update commands
                7. Create plan
                8. Assert plan output is as expected
            Result:
                Lock and unlock tasks are present in plan output
                after creating a test software item and linking
                it to nodes under HA management
        """
        #ha_manager_prop_updated = False
        #software_path = self.get_software_path(self.plugin_id)
        #node_url = self.find(self.test_ms, "/deployments", "node")[0]

        # 1. copy over and install RPMs
        #self.assertTrue(self.install_rpms())

        #try:
            # update ha_manager property
            #self.assertTrue(self.update_ha_manager_prop("vcs"))
            #ha_manager_prop_updated = True

            # 2. execute create CLI command
            #self.execute_cli_create_cmd(
                #self.test_ms, software_path,
                #self.plugin_id, props="name=test_01"
            #)

            # the test plugin code queries the node context, so link this
            # new software item with new node items
            #url_to_link = os.path.join(node_url, "items", self.plugin_id)
            #self.execute_cli_inherit_cmd(
                #self.test_ms, url_to_link, software_path
            #)

            # create strings as expected output in plan std_out
            #lock_str, unlock_str = self.get_lock_unlock_strings([node_url])

            # 3. execute create_plan CLI command
            #self.execute_cli_createplan_cmd(self.test_ms)

            # 4. assert that lock/unlock tasks for the nodes are found
            #   in the plan output
            #std_out, _, _ = self.execute_cli_showplan_cmd(self.test_ms)

            # check that lock task strings are present
            #self.log("info", "Is {0} in {1}?".format(lock_str, std_out))
            #self.assertTrue(self.is_text_in_list(lock_str, std_out))

            # check that unlock task strings are present
            #self.log("info", "Is {0} in {1}?".format(unlock_str, std_out))
            #self.assertTrue(self.is_text_in_list(unlock_str, std_out))

            # assert that lock/unlock tasks are in correct order
            #self.assertTrue(
                #self.lock_unlock_tasks_correct_order(std_out, [node_url])
            #)

            # 5. remove previous plan
            #self.execute_cli_removeplan_cmd(self.test_ms)

            # 6. execute update CLI command on the new software item
            #self.execute_cli_update_cmd(
                #self.test_ms, software_path, props="name=test_updated"
            #)

            # execute update CLI command on the node items as well
            #url_to_link = os.path.join(node_url, "items", self.plugin_id)
            #self.execute_cli_update_cmd(
                #self.test_ms, url_to_link, props="name=test_updated"
            #)

            # 7. execute create_plan CLI command
            #self.execute_cli_createplan_cmd(self.test_ms)

            # 8. assert plan output is as expected
            #std_out, _, _ = self.execute_cli_showplan_cmd(self.test_ms)

            # check that lock task strings are present
            #self.log("info", "Is {0} in {1}?".format(lock_str, std_out))
            #self.assertTrue(self.is_text_in_list(lock_str, std_out))

            # check that unlock task strings are present
            #self.log("info", "Is {0} in {1}?".format(unlock_str, std_out))
            #self.assertTrue(self.is_text_in_list(unlock_str, std_out))

            # assert that lock/unlock tasks are in correct order
            #self.assertTrue(
                #self.lock_unlock_tasks_correct_order(std_out, [node_url])
            #)
        #finally:
            # if the HA_Manager property was updated, reset it
            #if ha_manager_prop_updated:
                #self.assertTrue(
                    #self.update_ha_manager_prop(reset=True),
                    #"Unable to reset the value of the HA_Manager property"
                #)

    #attr('all', 'non-revert')
    def obsolete_02_p_no_lock_unlock(self):
        """
            Description:
                Given a LITP deployment if a new node is added to the cluster
                and it's in state Initial, then no lock/unlock tasks must be
                generated by the plugin for that node
            Actions:
                1. Copy over and install test RPMS
                2. Add a new node
                3. Run create + link commands
                4. Run create_plan
                5. Assert that the output is as expected
            Result:
                The new node will not have any lock/unlock tasks in
                the plan output
        """
        # AFTER FURTHER INVESTIGATION DUE TO THIS FAILING FOR SOME TIME, IT
        # SEEMS THIS WILL BE IMPOSSIBLE TO EXECUTE ALSO - MARKING OBSOLETE
        #ha_manager_prop_updated = False
        #software_path = self.get_software_path(self.plugin_id)
        #node_urls = self.find(self.test_ms, "/deployments", "node")
        #test_node_name = "nodeXYZ"

        # 1. copy over and install RPMs
        #self.assertTrue(self.install_rpms())

        #try:
            # update ha_manager property
            #self.assertTrue(self.update_ha_manager_prop("vcs"))

            # 2. add a node
            #results = self.run_commands(
                #self.test_ms,
                #self.get_create_node_deploy_cmds(
                    #self.test_ms, node_name=test_node_name,
                    #hostname=test_node_name
                #)
            #)
            #self.assertEquals([], self.get_errors(results))

            # 3. execute create CLI command
            #self.execute_cli_create_cmd(
                #self.test_ms, software_path,
                #self.plugin_id, props="name=test_02"
            #)

            # the test plugin code queries the node context, so link this
            # new software item with new node items
            #for node_url in node_urls:
               #if test_node_name in node_url:
                    #url_to_link = os.path.join(
                        #node_url, "items", self.plugin_id)
                    #self.execute_cli_inherit_cmd(
                        #self.test_ms, url_to_link, software_path
                    #)

            # 4. execute create_plan CLI command
            #self.execute_cli_createplan_cmd(self.test_ms)

            # 5. assert that there are no lock/unlock tasks
            #std_out, _, _ = self.execute_cli_showplan_cmd(self.test_ms)

            # we are checking that the newly added node (in initial state)
            # does not get any lock/unlock tasks, so check that node_name
            # is not present in plan output
            #lock_str = "Lock task {0}".format(test_node_name)
            #unlock_str = "Unlock task {0}".format(test_node_name)
            #self.assertFalse(self.is_text_in_list(lock_str, std_out))
            #self.assertFalse(self.is_text_in_list(unlock_str, std_out))
        #finally:
            # if the HA_Manager property was updated, reset it
            #if ha_manager_prop_updated:
                #self.assertTrue(
                    #self.update_ha_manager_prop(reset=True),
                    #"Unable to reset the value of the HA_Manager property"
                #)

    @attr('all', 'non-revert', 'story1838', 'story1838_tc03')
    def test_03_p_no_lock_unlock_req(self):
        """
            Description:
                Given a LITP deployment, if a node is part of an HA_cluster
                and the plan contains at least one task for that node,
                if no plugin provides lock/unlock tasks then the node
                will not be locked.
            Actions:
                1. Copy over and install test RPMS
                2. Run create + link commands
                3. Run create_plan
                4. Assert that the output is as expected
            Result:
                The added item will not have any lock/unlock tasks in plan
                output
        """
        ha_manager_prop_updated = False
        software_path = self.get_software_path(self.plugin_id)
        node_url = self.find(self.test_ms, "/deployments", "node")[0]

        # 1. copy over and install RPMs
        self.install_rpms(self.test_ms,
                        LOCAL_PLUGINS_DIR, self.plugin_id)

        try:
            self.assertTrue(self.update_ha_manager_prop("vcs"))

            # 2. execute create CLI command
            self.execute_cli_create_cmd(
                self.test_ms, software_path,
                self.plugin_id, props="name=test_03"
            )

            # the test plugin code queries the node context, so link this
            # new software item with a new node item
            url_to_link = os.path.join(node_url, "items", self.plugin_id)
            self.execute_cli_inherit_cmd(
                self.test_ms, url_to_link, software_path
            )

            # create strings as expected output in plan std_out
            lock_str, unlock_str = self.get_lock_unlock_strings([node_url])

            # 3. execute create_plan CLI command
            self.execute_cli_createplan_cmd(self.test_ms)

            # 4. assert that there are no lock/unlock tasks (were not required)
            std_out, _, _ = self.execute_cli_showplan_cmd(self.test_ms)

            # check that lock task strings are NOT present
            self.assertFalse(self.is_text_in_list(lock_str, std_out))

            # check that unlock task strings are NOT present
            self.assertFalse(self.is_text_in_list(unlock_str, std_out))
        finally:
            # if the HA_Manager property was updated, reset it
            if ha_manager_prop_updated:
                self.assertTrue(
                    self.update_ha_manager_prop(reset=True),
                    "Unable to reset the value of the HA_Manager property"
                )

    @attr('all', 'non-revert', 'story1838', 'story1838_tc04')
    def test_04_n_lock_no_unlock(self):
        """
            Description:
                Given a LITP deployment, if a node is part of a HA_cluster
                and the plan contains at least one task for that node, if a
                plugin provides a lock task but no unlock task, then the
                create_plan command should return an error.
            Actions:
                1. Copy over and install test RPMS
                2. Run create + link commands
                3. Run create_plan
                4. Assert plan failure + correct error message
            Result:
                The plan creation attempt will fail with a reported error
                that the plugin has a lock task with no associated unlock task
        """
        ha_manager_prop_updated = False
        software_path = self.get_software_path(self.plugin_id)
        node_url = self.find(self.test_ms, "/deployments", "node")[0]

        # 1. copy over and install RPMs
        self.install_rpms(self.test_ms,
                      LOCAL_PLUGINS_DIR, self.plugin_id)

        try:
            self.assertTrue(self.update_ha_manager_prop("vcs"))

            # 2. execute create CLI command
            self.execute_cli_create_cmd(
                self.test_ms, software_path,
                self.plugin_id, props="name=test_04"
            )

            # the test plugin code queries the node context, so link this
            # new software item with a new node item
            url_to_link = os.path.join(node_url, "items", self.plugin_id)
            self.execute_cli_inherit_cmd(
                self.test_ms, url_to_link, software_path
            )
            start_log_position = self.get_file_len(self.test_ms, self.log_path)
            # 3. execute create_plan CLI command
            _, std_err, _ = self.execute_cli_createplan_cmd(
                self.test_ms, expect_positive=False
            )
            end_log_position = self.get_file_len(self.test_ms, self.log_path)
            test_log_len = end_log_position - start_log_position
            err_type = "InternalServerError"
            self.assertTrue(self.is_text_in_list(err_type, std_err))
            # 4. assert correct error message is present
            self._get_expected_log_message('Validation errors', test_log_len)
        finally:
            # if the HA_Manager property was updated, reset it
            if ha_manager_prop_updated:
                self.assertTrue(
                    self.update_ha_manager_prop(reset=True),
                    "Unable to reset the value of the HA_Manager property"
                )

    @attr('all', 'non-revert', 'story1838', 'story1838_tc05')
    def test_05_n_unlock_no_lock(self):
        """
            Description:
                Given a LITP deployment, if a node is part of a HA_cluster
                and the plan contains at least one task for that node,
                if a plugin provides an unlock task but no lock task,
                then the create_plan command should return an error.
            Actions:
                1. Copy over and install test RPMS
                2. Run create + link commands
                3. Run create_plan
                4. Assert that a correct error message is present
            Result:
                The plan creation attempt will fail with a reported error
                that the plugin has an unlock task with no associated lock task
        """
        ha_manager_prop_updated = False
        software_path = self.get_software_path(self.plugin_id)
        node_url = self.find(self.test_ms, "/deployments", "node")[0]

        # 1. copy over and install RPMs
        self.install_rpms(self.test_ms,
                     LOCAL_PLUGINS_DIR, self.plugin_id)

        try:
            self.assertTrue(self.update_ha_manager_prop("vcs"))

            # 2. execute create CLI command
            self.execute_cli_create_cmd(
                self.test_ms, software_path,
                self.plugin_id, props="name=test_05"
            )

            # the test plugin code queries the node context, so link this
            # new software item with a new node item
            url_to_link = os.path.join(node_url, "items", self.plugin_id)
            self.execute_cli_inherit_cmd(
                self.test_ms, url_to_link, software_path
            )
            start_log_position = self.get_file_len(self.test_ms, self.log_path)
            # 3. execute create_plan CLI command
            _, std_err, _ = self.execute_cli_createplan_cmd(
                self.test_ms, expect_positive=False
            )
            end_log_position = self.get_file_len(self.test_ms, self.log_path)
            test_log_len = end_log_position - start_log_position
            err_type = "InternalServerError"
            self.assertTrue(self.is_text_in_list(err_type, std_err))
            # 4. assert correct error message is present
            self._get_expected_log_message('Validation errors', test_log_len)
        finally:
            # if the HA_Manager property was updated, reset it
            if ha_manager_prop_updated:
                self.assertTrue(
                    self.update_ha_manager_prop(reset=True),
                    "Unable to reset the value of the HA_Manager property"
                )

    @attr('all', 'non-revert', 'story1838', 'story1838_tc06')
    def test_06_n_multitple_plugins_ret(self):
        """
            Description:
                Given a LITP deployment, if a node is part of a HA_cluster
                and the plan contains at least one task for that node,
                if more then one plugin return lock/unlock tasks,
                create_plan should return an error.
            Actions:
                1. Copy over and install test RPMS
                2. Run create + link commands
                3. Run create_plan and assert failure + correct error message
            Result:
                The plan creation attempt will fail with a reported error
                that multiple plugins are returning lock/unlock tasks.
        """
        ha_manager_prop_updated = False
        software_path = self.get_software_path(self.plugin_id)
        node_url = self.find(self.test_ms, "/deployments", "node")[0]

        # 1. copy over and install RPMs
        self.install_rpms(
                      self.test_ms, LOCAL_PLUGINS_DIR, self.plugin_id)

        try:
            self.assertTrue(self.update_ha_manager_prop("vcs"))
            # 3. execute create CLI command
            self.execute_cli_create_cmd(
                self.test_ms, software_path,
                self.plugin_id, props="name=test_06 second_plugin=true"
            )

            # the test plugin code queries the node context, so link this
            # new software item with a new node item
            url_to_link = os.path.join(node_url, "items", self.plugin_id)
            self.execute_cli_inherit_cmd(
                self.test_ms, url_to_link, software_path,
            )
            start_log_position = self.get_file_len(self.test_ms, self.log_path)
            # 3. execute create_plan CLI command
            _, std_err, _ = self.execute_cli_createplan_cmd(
                self.test_ms, expect_positive=False
            )
            end_log_position = self.get_file_len(self.test_ms, self.log_path)
            test_log_len = end_log_position - start_log_position
            err_type = "InternalServerError"
            self.assertTrue(self.is_text_in_list(err_type, std_err))
            # 4. assert correct error message is present
            self._get_expected_log_message('Validation errors', test_log_len)
        finally:
            # if the HA_Manager property was updated, reset it
            if ha_manager_prop_updated:
                self.assertTrue(
                    self.update_ha_manager_prop(reset=True),
                    "Unable to reset the value of the HA_Manager property"
                )

    @attr('all', 'non-revert', 'story1838', 'story1838_tc07')
    def test_07_n_no_hamanager(self):
        """
            Description:
                Given a LITP deployment, if a node is part of a HA_cluster
                and the plan contains at least one task for that node,
                if the plugin provides lock/unlock tasks for a different
                HA_Manager, then the plan output will have no node
                lock/unlock tasks.

                Note. The plugin code looks for a dummy ha_manager
                value so this test runs without any updates to the
                current ha_manager for the cluster
            Actions:
                1. Copy over and install test RPMS
                2. Run create + link commands
                3. Run create_plan
                4. Assert that the output is as expected
            Result:
                The plan output will not contain any node lock/unlock
                tasks for the nonexistent HA_Manager
        """
        software_path = self.get_software_path(self.plugin_id)
        node_url = self.find(self.test_ms, "/deployments", "node")[0]

        # 1. copy over and install RPMs
        self.install_rpms(self.test_ms, LOCAL_PLUGINS_DIR, self.plugin_id)

        # 2. execute create + link CLI command
        self.execute_cli_create_cmd(
            self.test_ms, software_path,
            self.plugin_id, props="name=test_07"
        )

        # the test plugin code queries the node context, so link this
        # new software item with a new node item
        url_to_link = os.path.join(node_url, "items", self.plugin_id)
        self.execute_cli_inherit_cmd(
            self.test_ms, url_to_link, software_path
        )

        # create strings as expected output in plan std_out
        lock_str, unlock_str = self.get_lock_unlock_strings([node_url])

        # 3. execute create_plan CLI command
        self.execute_cli_createplan_cmd(self.test_ms)

        # 4. assert that there are no lock/unlock tasks
        std_out, _, _ = self.execute_cli_showplan_cmd(self.test_ms)

        # check that lock task strings are NOT present
        self.assertFalse(self.is_text_in_list(lock_str, std_out))

        # check that unlock task strings are NOT present
        self.assertFalse(self.is_text_in_list(unlock_str, std_out))

    #attr('all', 'non-revert')
    def obsolete_08_p_lock_unlock_ordered(self):
        """
            Description:
                Given a LITP deployment, if a node is part of a HA_cluster
                and the plan contains at least one task for that node,
                if the plugin provides lock/unlock tasks and an ordered
                task list, then lock/unlock tasks will be ordered correctly
                in relation to the task lists.
            Actions:
                1. Copy over and install test RPMS
                2. Run create + link commands
                3. Run create_plan
                4. Assert that the output is as expected
            Result:
                Lock and unlock tasks are present in the plan output
                and are correctly positioned between the tasks
        """
        #ha_manager_prop_updated = False
        #software_path = self.get_software_path(self.plugin_id)
        #node_urls = self.find(self.test_ms, "/deployments", "node")

        # 1. copy over and install RPMs
        #self.assertTrue(self.install_rpms())

        #try:
            #self.assertTrue(self.update_ha_manager_prop("vcs"))
            # 3. execute create CLI command
            #self.execute_cli_create_cmd(
                #self.test_ms, software_path,
                #self.plugin_id, props="name=test_08"
            #)

            # the test plugin code queries the node context, so link this
            # new software item with a new node item
            #for node_url in node_urls:
                #url_to_link = os.path.join(node_url, "items", self.plugin_id)
                #self.execute_cli_inherit_cmd(
                    #self.test_ms, url_to_link, software_path
                #)

            # create strings as expected output in plan std_out
            #lock_strs, unlock_strs = self.get_lock_unlock_strings(node_urls)

            # 4. execute create_plan CLI command
            #self.execute_cli_createplan_cmd(self.test_ms)

            # 5. assert that an error for missing lock task is present
            #std_out, _, _ = self.execute_cli_showplan_cmd(self.test_ms)

            # check that lock task strings are present
            #self.log("info", "Are {0} in {1}?".format(lock_strs, std_out))
            #self.assertTrue(
                #all([
                    #self.is_text_in_list(lock_str, std_out)
                    #for lock_str in lock_strs
                #])
            #)

            # check that unlock task strings are present
            #self.log("info", "Are {0} in {1}?".format(unlock_strs, std_out))
            #self.assertTrue(
                #all([
                    #self.is_text_in_list(unlock_str, std_out)
                    #for unlock_str in unlock_strs
                #])
            #)

            # check the lock / unlock tasks are in correct ordering
            # we care only about lock/unlock task ordering, the internal
            # order of the OrderedTaskList is irrelevant
            #self.assertTrue(
                #self.lock_unlock_tasks_correct_order(std_out, node_urls)
            #)
        #finally:
            # if the HA_Manager property was updated, reset it
            #if ha_manager_prop_updated:
                #self.assertTrue(
                    #self.update_ha_manager_prop(reset=True),
                    #"Unable to reset the value of the HA_Manager property"
                #)

    #attr('all', 'non-revert')
    def obsolete_09_p_rpctask_expansion(self):
        """
            Description:
                Given a LITP deployment, if a node is part of a HA_cluster
                and the plan contains at least one task for that node,
                if the plugin provides lock/unlock tasks and an remote
                execution task, then the plan output will return three
                remote execution tasks, one per node, with
                lock/unlock tasks present as well
            Actions:
                1. Copy over and install test RPMS
                2. Run create + link commands
                3. Run create_plan
                4. Assert that the output is as expected
            Result:
                The returned RemoteExecutionTask() is actually expanded to
                multiple tasks, one per node with associated lock/unlock
                tasks
        """
        #ha_manager_prop_updated = False
        #software_path = self.get_software_path(self.plugin_id)
        #node_urls = self.find(self.test_ms, "/deployments", "node")

        # 1. copy over and install RPMs
        #self.assertTrue(self.install_rpms())

        #try:
            #self.assertTrue(self.update_ha_manager_prop("vcs"))
            # 3. execute create CLI command
            #self.execute_cli_create_cmd(
                #self.test_ms, software_path,
                #self.plugin_id, props="name=test_09"
            #)

            # the test plugin code queries the node context, so link this
            # new software item with a new node item
            #for node_url in node_urls:
                #url_to_link = os.path.join(node_url, "items", self.plugin_id)
                #self.execute_cli_inherit_cmd(
                    #self.test_ms, url_to_link, software_path,
                #)

            # create strings as expected output in plan std_out
            #lock_strs, unlock_strs = self.get_lock_unlock_strings(node_urls)

            # 4. execute create_plan CLI command
            #self.execute_cli_createplan_cmd(self.test_ms)

            # 5. assert that there are no lock/unlock tasks (were not required)
            #std_out, _, _ = self.execute_cli_showplan_cmd(self.test_ms)

            # check that lock task strings are present
            #self.log("info", "Are {0} in {1}?".format(lock_strs, std_out))
            #self.assertTrue(
                #all([
                    #self.is_text_in_list(lock_str, std_out)
                    #for lock_str in lock_strs
                #])
            #)

            # check that unlock task strings are present
            #self.log("info", "Are {0} in {1}?".format(unlock_strs, std_out))
            #self.assertTrue(
                #all([
                    #self.is_text_in_list(unlock_str, std_out)
                    #for unlock_str in unlock_strs
                #])
            #)

            # check number of RemoteExecutionTasks
            #task_str = "RemoteExecutionTask() test_09"
            #num_nodes = len(self.get_managed_node_filenames())
            #task_count = 0

            # count number of RemoteExecutionTasks
            #for line in std_out:
                #if task_str in line:
                    #task_count += 1

            # check that we got enough RemoteExecutionTasks
            #self.assertTrue(task_count == num_nodes)

            # assert that the lock/unlock tasks come in the correct order
            #self.assertTrue(
                #self.lock_unlock_tasks_correct_order(std_out, node_urls)
            #)
        #finally:
            # if the HA_Manager property was updated, reset it
            #if ha_manager_prop_updated:
                #self.assertTrue(
                    #self.update_ha_manager_prop(reset=True),
                    #"Unable to reset the value of the HA_Manager property"
                #)

    @attr('all', 'non-revert', 'story1838', 'story1838_tc10')
    def test_10_n_cluster_lock_unlock(self):
        """
            Description:
                Given a LITP deployment, if a node is part of a HA_cluster
                and the plan contains at least one task for that node,
                if the plugin provides lock/unlock tasks for the cluster
                level, then LITP will report an error.
            Actions:
                1. Copy over and install test RPMS
                2. Run create + link commands
                3. Run create_plan and assert failure
            Result:
                LITP returns an error that the cluster
                cannot be locked/unlocked
        """
        ha_manager_prop_updated = False
        software_path = self.get_software_path(self.plugin_id)
        node_url = self.find(self.test_ms, "/deployments", "node")[0]

        # 1. copy over and install RPMs
        self.install_rpms(self.test_ms,
                         LOCAL_PLUGINS_DIR, self.plugin_id)

        try:
            self.assertTrue(self.update_ha_manager_prop("vcs"))
            # 3. execute create CLI command
            self.execute_cli_create_cmd(
                self.test_ms, software_path,
                self.plugin_id, props="name=test_10"
            )

            # the test plugin code queries the node context, so link this
            # new software item with a new node item
            url_to_link = os.path.join(node_url, "items", self.plugin_id)
            self.execute_cli_inherit_cmd(
                self.test_ms, url_to_link, software_path
            )

            # 4. execute create_plan CLI command
            _, _, _ = self.execute_cli_createplan_cmd(
                self.test_ms, expect_positive=False
            )
        finally:
            # if the HA_Manager property was updated, reset it
            if ha_manager_prop_updated:
                self.assertTrue(
                    self.update_ha_manager_prop(reset=True),
                    "Unable to reset the value of the HA_Manager property"
                )

    #attr('all', 'non-revert')
    def obsolete_11_p_all_task_types(self):
        """
            Description:
                Given a LITP deployment, if a node is part of a HA_cluster
                and the plan contains at least one task for that node,
                if the plugin provides lock/unlock tasks, then the task(s)
                will be executed while the node is locked (the node will
                not necessarily be unlocked immediately
                after the task(s) is executed).
            Actions:
                1. Copy over and install test RPMS
                3. Run create + link commands
                4. Run create_plan
                5. Assert that the output is as expected
            Result:
                Lock/Unlock tasks are present in plan output
        """
        #ha_manager_prop_updated = False
        #software_path = self.get_software_path(self.plugin_id)
        #node_url = self.find(self.test_ms, "/deployments", "node")[0]

        # 1. copy over and install RPMs
        #self.assertTrue(self.install_rpms())

        #try:
            #self.assertTrue(self.update_ha_manager_prop("vcs"))
            # 3. execute create CLI command
            #self.execute_cli_create_cmd(
                #self.test_ms, software_path,
                #self.plugin_id, props="name=test_11"
            #)

            # the test plugin code queries the node context, so link this
            # new software item with a new node item
            #url_to_link = os.path.join(node_url, "items", self.plugin_id)
            #self.execute_cli_inherit_cmd(
                #self.test_ms, url_to_link, software_path
            #)

            # 4. execute create_plan CLI command
            #self.execute_cli_createplan_cmd(self.test_ms)

            # 5. assert that plan output is as expected
            #std_out, _, _ = self.execute_cli_showplan_cmd(self.test_ms)

            # create strings as expected output in plan std_out
            #lock_str, unlock_str = self.get_lock_unlock_strings([node_url])

            # check that lock task strings are present
            #self.assertTrue(self.is_text_in_list(lock_str, std_out))

            # check that unlock task strings are present
            #self.assertTrue(self.is_text_in_list(unlock_str, std_out))
        #finally:
            # if the HA_Manager property was updated, reset it
            #if ha_manager_prop_updated:
                #self.assertTrue(
                    #self.update_ha_manager_prop(reset=True),
                    #"Unable to reset the value of the HA_Manager property"
                #)
